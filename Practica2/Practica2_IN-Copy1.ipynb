{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/envs/IN_/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/home/ismael/anaconda3/envs/IN_/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colisión accidentes:  3222\n",
      "Colisión accidentes:  169\n",
      "Colisión accidentes:  3334\n",
      "\n",
      "Caso de estudio 1:\n",
      "\n",
      "Ward               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/envs/IN_/lib/python3.6/site-packages/sklearn/cluster/hierarchical.py:193: UserWarning: the number of connected components of the connectivity matrix is 4 > 1. Completing it to avoid stopping the tree early.\n",
      "  affinity='euclidean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": k:   8,  37.32 segundos, CH Index: 0.000000000, SH: 0.00000\n",
      "k_filtrado:  8\n",
      "---------- Preparando el scatter matrix...\n",
      "k-means            : k:   4,   0.04 segundos, CH Index: 17961.363341743, SH: 0.94660\n",
      "k_filtrado:  4\n",
      "---------- Preparando el scatter matrix...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from math import floor\n",
    "\n",
    "\n",
    "#Activar para relizar pruebas con menos Datos\n",
    "prueba = False\n",
    "\n",
    "\n",
    "\n",
    "accidentes = pd.read_csv('accidentes_2013.csv')\n",
    "\n",
    "if(prueba):\n",
    "    accidentes = accidentes.sample(len(accidentes)//10)\n",
    "    print(len(accidentes))\n",
    "else:\n",
    "    print(len(accidentes))\n",
    "\n",
    "\n",
    "#seleccionar accidentes de tipo 'colisión de vehículos'\n",
    "subset1 = accidentes[accidentes['TIPO_ACCIDENTE'].str.contains(\"Colisión de vehículos\")]\n",
    "subset1 = subset1[accidentes['TIPO_ACCIDENTE'].str.contains(\"(Alcance)\") & (accidentes['TIPO_VIA'].str.contains(\"AUTOPISTA\") | accidentes['TIPO_VIA'].str.contains(\"AUTOVÍA\"))]\n",
    "caso_estudio1 = ['HORA', 'DIASEMANA', 'TOT_VICTIMAS']\n",
    "X1 = subset1[caso_estudio1]\n",
    "print('Colisión accidentes: ',len(X1))\n",
    "subset2 = accidentes[accidentes['TIPO_ACCIDENTE'].str.contains(\"Atropello\")& accidentes['PROVINCIA'].str.contains(\"Córdoba\")]\n",
    "# subset2 = subset2.loc[(accidentes['DIASEMANA']>=5) & (accidentes['DIASEMANA']<=7)]\n",
    "caso_estudio2 = ['MES', 'DIASEMANA','HORA', 'TOT_MUERTOS']\n",
    "X2 = subset2[caso_estudio2]\n",
    "print('Colisión accidentes: ',len(X2))\n",
    "subset3 = accidentes[accidentes['TIPO_ACCIDENTE'].str.contains(\"Vuelco en la calzada\")]\n",
    "caso_estudio3 = ['TOT_HERIDOS_GRAVES', 'TOT_HERIDOS_LEVES', 'TOT_VEHICULOS_IMPLICADOS']\n",
    "X3 = subset3[caso_estudio3]\n",
    "print('Colisión accidentes: ',len(X3))\n",
    "\n",
    "usadas = [caso_estudio1,caso_estudio2,caso_estudio3]\n",
    "casos_de_estudio = [X3]\n",
    "\n",
    "k_means = cluster.KMeans(init='k-means++',n_clusters=4, n_init=5)\n",
    "# mbkm = cluster.MiniBatchKMeans(n_clusters=4)\n",
    "ms = cluster.MeanShift()\n",
    "spectral = cluster.SpectralClustering(n_clusters=8)#arpack\n",
    "# affinity_propagation = cluster.AffinityPropagation()\n",
    "dbscan = cluster.DBSCAN(eps=0.1, min_samples=20 )\n",
    "birch = cluster.Birch(n_clusters=6,threshold=0.2)\n",
    "# Define the structure A of the data. Here a 10 nearest neighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "connectivity = kneighbors_graph(X3, n_neighbors=10, include_self=False)\n",
    "ward = cluster.AgglomerativeClustering(n_clusters=8, connectivity=connectivity)\n",
    "\n",
    "clustering_algorithm = (\n",
    "#     ('k-means',k_means),\n",
    "# #     ('MiniBatkMeans',mbkm),\n",
    "    # ('MeanShift',ms),\n",
    "#     ('DBSCAN',dbscan)\n",
    "#     ('Birch',birch)\n",
    "#     ('SpectralClustering',spectral)\n",
    "    ('Ward',ward),\n",
    "    ('k-means',k_means)\n",
    ")\n",
    "\n",
    "for i,X in enumerate(casos_de_estudio):\n",
    "    print(\"\\nCaso de estudio \"+str(i+1)+\":\\n\")\n",
    "    X_normal = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "    for j,q in enumerate(clustering_algorithm):\n",
    "\n",
    "        name,algorithm = q\n",
    "        print('{:19s}'.format(name),end='')\n",
    "        t = time.time()\n",
    "        cluster_predict = algorithm.fit_predict(X_normal)\n",
    "        # print('\\nprediccion: ',cluster_predict)\n",
    "        tiempo = time.time() - t\n",
    "        k = len(set(cluster_predict))\n",
    "        print(\": k: {:3.0f}, \".format(k),end='')\n",
    "        print(\"{:6.2f} segundos, \".format(tiempo),end='')\n",
    "\n",
    "        if(k>1) and (name is not 'Ward'):\n",
    "            metric_CH = metrics.calinski_harabaz_score(X_normal, cluster_predict)\n",
    "\n",
    "            metric_SH = metrics.silhouette_score(X_normal, cluster_predict,metric = 'euclidean',sample_size=floor(0.1*len(X)),random_state=123456)\n",
    "        else:\n",
    "            metric_CH = 0\n",
    "            metric_SH = 0\n",
    "        print(\"CH Index: {:8.9f}, \".format(metric_CH),end='')\n",
    "        print(\"SH: {:.5f}\".format(metric_SH))\n",
    "\n",
    "        #se convierte la asignación de clusters a DataFrame\n",
    "        clusters = pd.DataFrame(cluster_predict,index=X.index,columns=['cluster'])\n",
    "        # print('cluster: ',cluster)\n",
    "        #y se añade como columna a X\n",
    "\n",
    "        X_cluster = pd.concat([X, clusters], axis=1)\n",
    "        #Filtro quitando los elementos (outliers) que caen en clusters muy pequeños en el jerárquico\n",
    "        min_size = 3\n",
    "        X_filtrado = X_cluster[X_cluster.groupby('cluster').cluster.transform(len) > min_size]\n",
    "        k_filtrado = len(set(X_filtrado['cluster']))\n",
    "        print('k_filtrado: ',k_filtrado)\n",
    "        # print(\"De los {:.0f} clusters hay {:.0f} con más de {:.0f} elementos. Del total de {:.0f} elementos, se seleccionan {:.0f}\".format(k['Ward'],k_filtrado,min_size,len(X),len(X_filtrado)))\n",
    "\n",
    "        if(i==1):\n",
    "            _X_filtrado_sin = X_filtrado.drop('cluster', 1)\n",
    "            #Normalizo el conjunto filtrado\n",
    "            _X_filtrado_normal = preprocessing.normalize(_X_filtrado_sin, norm='l2')\n",
    "            #Ahora lo saco usando seaborn (que a su vez usa scipy) para incluir un heatmap\n",
    "            import seaborn as sns\n",
    "            _X_filtrado_normal_DF = pd.DataFrame(_X_filtrado_normal,index=_X_filtrado_sin.index,columns=usadas[i])\n",
    "            dend_plot = sns.clustermap(_X_filtrado_normal_DF, method='ward', col_cluster=False, figsize=(20,10), cmap=\"YlGnBu\")\n",
    "            dend_plot.savefig('imagenes/dendograma.png')\n",
    "\n",
    "\n",
    "        # print(\"---------- Preparando el heatmaps...\")\n",
    "        # import seaborn as sns\n",
    "        # sns.set()\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "        # variables = xxx.copy()\n",
    "        # variables.pop('cluster')\n",
    "        # heat_plot = sns.heatmap(variables)\n",
    "        # plt.savefig('imagenes/Caso de estudio_'+str(i)+'_'+clustering_algorithm[j][0]+'_heat'+\".png\")\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "        #\n",
    "        # print(\"---------- Preparando el dendograma...\")\n",
    "        # sns.set()\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "        # variables_ = xxx.copy()\n",
    "        # variables_.pop('cluster')\n",
    "        # dend_plot = sns.clustermap(variables_)\n",
    "        # dend_plot.savefig('imagenes/Caso de estudio_'+str(i)+'_'+clustering_algorithm[j][0]+'_dendog'+\".png\")\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "        #\n",
    "        print(\"---------- Preparando el scatter matrix...\")\n",
    "        import seaborn as sns\n",
    "        sns.set()\n",
    "        variables = list(X_filtrado)\n",
    "        variables.remove('cluster')\n",
    "        sns_plot = sns.pairplot(X_filtrado, vars=variables, hue=\"cluster\", palette='Paired', plot_kws={\"s\": 25}, diag_kind=\"hist\") #en hue indicamos que la columna 'cluster' define los colores\n",
    "        sns_plot.fig.subplots_adjust(wspace=.03, hspace=.03);\n",
    "        sns_plot.savefig('imagenes/Caso_de_estudio_2/'+clustering_algorithm[j][0]+\"13.png\")\n",
    "        #\n",
    "        #\n",
    "        # print(\"\")\n",
    "        # #'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IN_",
   "language": "python",
   "name": "in_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
